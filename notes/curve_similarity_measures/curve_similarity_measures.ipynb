{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(curve_similarity_measures)=\n",
    "# Curve Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Suppose we have a curve representation method, for example some parameterization scheme such as a\n",
    "spline. We also have a particular shape specified (say the [Stanford Bunny](#stanford_bunny)) and we\n",
    "want our parameterization to represent that shape as closely as possible. To represent the given\n",
    "shape one way would be to tune the parameters of the representation scheme iteratively using a\n",
    "gradient descent optimization approach. This requires us to define an objective function that can\n",
    "then be optimized over. An appropriate objective for this task would be some measure of\n",
    "similarity(or dissimilarity) between the target curve and the one traced out by our parameterization\n",
    ". The objective function can then be maximized(or minimized) to fit the parameters.\n",
    "\n",
    "The target of this tutorial is to study **curve similarity measures**. We discuss different kinds of\n",
    "measures and also see their implementations.\n",
    "\n",
    ":::::{grid} 2\n",
    "\n",
    "::::{grid-item}\n",
    ":::{figure} assets/a_basic_spline.svg\n",
    ":label: a_basic_spline\n",
    ":width: 100%\n",
    ":alt: A basic spline drawing to represent a parameterized shape.\n",
    "Parameterized shape, using a spline.\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{grid-item}\n",
    ":::{figure} assets/stanford_bunny.svg\n",
    ":label: stanford_bunny\n",
    ":width: 60%\n",
    ":alt: The Stanford Bunny shown as a target curve.\n",
    "Target shape, the [Stanford Bunny](https://en.wikipedia.org/wiki/Stanford_bunny).\n",
    ":::\n",
    "::::\n",
    "\n",
    ":::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concrete Problem Setup\n",
    "First we define the different objects that we deal with:\n",
    "\n",
    "Shape Parameterization\n",
    ": We use some parameterization scheme e.g. splines to represent our shapes. We have a set of\n",
    "parameters $\\phi$ that represent our shape. By changing $\\phi$ we trace out different curves in the\n",
    "plane. We will think of $\\phi$ as a column vector $[\\phi_1, \\phi_2, \\ldots, \\phi_n]^{T}$.\n",
    "\n",
    "Parameterized Curve\n",
    ": This is the curve that is traced out by the parameterization scheme. We denote it by $C_p$ and is\n",
    "obtained by sampling the scheme at different points along the actual curve. It is specified in the\n",
    "form of a $N_p$ length sequence of $(x, y)$ points. These points are ordered along the curve. We\n",
    "will specify the points in a matrix in $\\mathbb{R}^{N_p \\times 2}$ where each row corresponds to a\n",
    "point $(x_i, y_i)$. We denote the matrix as $X_p$.\n",
    "\n",
    "Target Curve\n",
    ": This is the curve that we want our parameterization scheme to represent. We denote it by $C_t$ and\n",
    "it is specified in the form of a $N_t$ length sequence of $(x, y)$ points. These points are ordered\n",
    "along the curve. We will specify the points in a matrix in $\\mathbb{R}^{N_t \\times 2}$ as we did for\n",
    "the parameterized curve. We denote the matrix as $X_t$.\n",
    "\n",
    "Loss Function\n",
    ": A function denoted as $\\mathcal{L}(X_t, X_p)$ that measures the degree of dissimilarity between\n",
    "the target curve and the parameterized curve. It should be differentiable to allow us to find\n",
    "gradients $\\frac{d\\mathcal{L}}{d\\phi}$ that can then be used to run gradient descent.\n",
    "\n",
    "**_Goal_**: To tune $\\phi$ such that our representation scheme traces out the target curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measures\n",
    "We now discuss the different curve similarity measures. For each measure we describe the exact\n",
    "mathematical definition, practical considerations, modifications to make them differentiable and\n",
    "implementations in [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    ":::{note} Assumption\n",
    "$N_p = N_t = N$. That is, we sample the parameterized curve at exactly $N_t$ points.\n",
    ":::\n",
    "\n",
    "The mean squared error loss function computes the average of the squared distance between the\n",
    "corresponding points on the two curves. Mathematically,\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( d(X_{p}[i], X_{t}[i]) \\right)^2\n",
    "$$\n",
    "where, $X[i]$ denotes $i${sup}`th` row, i.e. $(x_i, y_i)$ and $d$ is a distance function.\n",
    "\n",
    "Though the measure is quite naive and not very robust, it is very simple and quick to implement and\n",
    "is also differentiable without any modifications.\n",
    "\n",
    ":::{figure} assets/mse_visualization.svg\n",
    ":width: 70%\n",
    ":alt: Two curves defined by an equal number of points and an MSE loss between corresponding points.\n",
    "\n",
    "Mean Squared Error (MSE) between two curves visualized.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def mse_loss(X_p, X_t):\n",
    "    # Calculate the squared Euclidean distances between corresponding rows\n",
    "    squared_distances = torch.sum((X_p - X_t) ** 2, dim = 1)\n",
    "\n",
    "    # Calculate mean of the squared distances to get the loss\n",
    "    loss = torch.mean(squared_distances)\n",
    "\n",
    "    return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Descriptor Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "The idea behind Fourier descriptor matching is to compute the Fourier coefficients of both the\n",
    "target and the parameterized curve and then use the difference between them as the loss function.\n",
    "\n",
    "Concretely, given a curve we can approximate it using a complex Fourier series as follows [^3b1b_FourierSeries_video]:\n",
    "$$\n",
    "X(t) = \\sum_{n = -\\infty}^{\\infty} c_n e^{n 2 \\pi i t} \\quad t \\in [0, 1)\n",
    "$$\n",
    "\n",
    "In Fourier descriptor matching we use the FFT algorithm to compute a finite number of coefficients\n",
    "$c_n$ for each of the curves which have themselves been sampled(from $X(t)$) at a finite number of\n",
    "points given by $X_p$ and $X_t$. Let the coefficients be defined in vectors $F_p$ and $F_t$. The\n",
    "loss is then computed as the mean squared error of the two coefficient vectors. If $k$ is the total\n",
    "Fourier coefficients computed in each FFT then the loss is given by:\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{k} \\sum_{i=1}^{k} \\left( d(F_{p}[i], F_{t}[i]) \\right)^2\n",
    "$$\n",
    "\n",
    ":::{note}\n",
    "Fourier Descriptor Matching works only for **closed curves**. For open curves, it approximates with\n",
    "a closed curve.\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "For the loss to be differentiable we require that the FFT be computed in a way that\n",
    "allows automatic differentiation to work.\n",
    ":::\n",
    "\n",
    "[^3b1b_FourierSeries_video]:\n",
    "    3Blue1Brown's video on Fourier Series explains the formula really well.\n",
    "    :::{iframe} https://www.youtube.com/embed/r6sGWTCMz2k?si=ISN2KdknXNlmSr4u\n",
    "    :::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.fft\n",
    "\n",
    "def fourier_descriptor_matching_loss(X_p, X_t, num_descriptors):\n",
    "    # Compute Fourier transforms (using FFT)\n",
    "    fft_p = torch.fft.fft(torch.complex(X_p[..., 0], X_p[..., 1]))\n",
    "    fft_t = torch.fft.fft(torch.complex(X_t[..., 0], X_t[..., 1]))\n",
    "\n",
    "    # Select relevant descriptors (low frequencies)\n",
    "    F_p = fft_p[:num_descriptors]\n",
    "    F_t = fft_t[:num_descriptors]\n",
    "\n",
    "    # Calculate MSE loss on magnitudes or complex values\n",
    "    loss = torch.mean(torch.abs(F_p - F_t)**2)\n",
    "    return loss\n",
    "```\n",
    "\n",
    "The implementation works because the FFT is differentiable in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hausdorff Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "**_Note_**: Most of the information is taken from the Wikipedia page\n",
    "[Hausdorff Distance](https://en.wikipedia.org/wiki/Hausdorff_distance).\n",
    "\n",
    "The Hausdorff distance measures how far two subsets of a metric space are from each other.\n",
    "Informally, two sets are close in the Hausdorff distance if every point of either set is close to\n",
    "some point of the other set. The Hausdorff distance is the longest distance someone can be forced to\n",
    "travel by an adversary who chooses a point in one of the two sets, from where they then must travel\n",
    "to the other set. In other words, it is the greatest of all the distances from a point in one set to\n",
    "the closest point in the other set.\n",
    "\n",
    "Let $(M, d)$ be a metric space. For each pair of non-empty subsets $X \\subset M$ and $Y \\subset M$,\n",
    "the Hausdorff distance between $X$ and $Y$ is defined as\n",
    "$$\n",
    "d_{\\mathrm H}(X,Y) := \\max\\left\\{\\,\\sup_{x \\in X} d(x,Y),\\ \\sup_{y \\in Y} d(X,y) \\,\\right\\}\n",
    "$$\n",
    "\n",
    "where $\\sup$ represents the supremum operator, $\\inf$ the infimum operator, and where\n",
    "$d(a, B) := \\inf_{b \\in B} d(a,b)$ quantifies the distance from a point $a \\in X$ to the subset $B\n",
    "\\subseteq X$.\n",
    "\n",
    ":::{figure} assets/Hausdorff_distance.svg\n",
    ":label: Hausdorff_distance\n",
    ":width: 55%\n",
    ":alt: Figure showing two simple closed curves and $\\sup d(x, Y)$ and $\\sup d(y, X)$.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differentiability\n",
    "The Hausdorff distance is by itself not differentiable if we implement using minimum and maximum\n",
    "functions. Therefore we need to work with approximations to it such as:\n",
    "\n",
    "Soft Hausdorff\n",
    ": Compute the approximate Hausdorff distance by smoothing the minimum operation to ensure\n",
    "differentiability. In this approach we use a [smooth minimum function](#smooth_min_max) instead of\n",
    "the minimum function directly.\n",
    "    1. Let $P_1$ and $P_2$ be the sets of points on the two curves.\n",
    "    2. Use a soft-minimum function to approximate the minimum distance between points, such as:\n",
    "    $$\n",
    "    d_{\\text{soft}}(p, P_2) = -\\tau \\log \\left( \\sum_{q \\in P_2} \\exp \\left( -\\frac{\\|p - q\\|_2^2}\n",
    "    {\\tau} \\right) \\right)\n",
    "    $$\n",
    "    3. Compute the smoothed Hausdorff distance:\n",
    "    $$\n",
    "    \\text{Hausdorff}_{\\text{soft}}(P_1, P_2) = \\frac{1}{|P_1|} \\sum_{p \\in P_1}\n",
    "    d_{\\text{soft}}(p, P_2) + \\frac{1}{|P_2|} \\sum_{q \\in P_2} d_{\\text{soft}}(q, P_1)\n",
    "    $$\n",
    "\n",
    "    :::{dropdown} LogSumExp - Smooth maximum\n",
    "    :label: smooth_min_max\n",
    "    The LogSumExp (LSE) function is a smooth maximum â€“ a smooth approximation to the maximum\n",
    "    function. It is defined as the logarithm of the sum of the exponentials of the arguments:\n",
    "    $$\n",
    "    \\mathrm{LSE}(x_1, \\ldots, x_n) = \\log\\left( \\exp(x_1) + \\cdots + \\exp(x_n) \\right)\n",
    "    $$\n",
    "\n",
    "    Writing $\\mathbf{x} = (x_1, \\ldots, x_n)$ the partial derivatives are:\n",
    "    $$\n",
    "    \\frac{\\partial}{\\partial x_i}{\\mathrm{LSE}(\\mathbf{x})} = \n",
    "    \\frac{\\exp x_i}{\\sum_j \\exp {x_j}}\n",
    "    $$\n",
    "    which means the gradient of LogSumExp is the softmax function.\n",
    "\n",
    "    Sometimes, a parameter $\\tau$ called the temperature parameter is used\n",
    "    $$\n",
    "    \\mathrm{LSE}(x_1, \\ldots, x_n) = \\tau \\log\\left( \\exp\\left(\\frac{x_1}{\\tau}\\right) + \\cdots +\n",
    "    \\exp\\left(\\frac{x_n}{\\tau}\\right) \\right)\n",
    "    $$\n",
    "    $\\tau$ controls the sharpness of the approximation. As $\\tau$ approaches 0, the softmin\n",
    "    approaches the true minimum.[^temperature_param]\n",
    "\n",
    "    Also, note that $\\min \\left( x, y \\right) = -\\max \\left(-x, -y \\right)$. We can use this to get\n",
    "    the smooth minimum function using the LogSumExp.\n",
    "    :::\n",
    "\n",
    "\n",
    "[^temperature_param]: $\\tau$ is called the temperature parameter because as $\\tau \\to \\infty$ too\n",
    "    much smoothing happens and all values tend to the same, i.e. at very high temperature behavior\n",
    "    is random. If we cool down $\\tau$ then the it tends to the true $\\max$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def hausdorff_soft_loss(X_p, X_t, tau = 1.0):\n",
    "    # Compute pairwise squared distances\n",
    "    dist_matrix = torch.cdist(P1, P2) ** 2  # Shape: (N1, N2)\n",
    "    \n",
    "    # Compute soft-minimum distances\n",
    "    d_soft_p1_to_p2 = -tau * torch.logsumexp(-dist_matrix / tau, dim = 1)  # Shape: (N1,)\n",
    "    d_soft_p2_to_p1 = -tau * torch.logsumexp(-dist_matrix.T / tau, dim = 1)  # Shape: (N2,)\n",
    "\n",
    "    # Compute the soft Hausdorff loss\n",
    "    hausdorff_soft = d_soft_p1_to_p2.mean() + d_soft_p2_to_p1.mean()\n",
    "    \n",
    "    return hausdorff_soft.item()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
