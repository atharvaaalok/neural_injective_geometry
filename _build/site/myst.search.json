{"version":"1","records":[{"hierarchy":{"lvl1":"Auxilliary Networks"},"type":"lvl1","url":"/auxilliary-networks","position":0},{"hierarchy":{"lvl1":"Auxilliary Networks"},"content":"We now introduce, Auxilliary Networks, additions to the basic Injective Network architecture that enhances their\nrepresentation power.\n\n# Basic imports\nimport torch\nfrom torch import nn\n\nfrom assets.shapes import square, circle, stanford_bunny\nfrom assets.loss_functions import mse\nfrom assets.utils import automate_training, plot_curves\n\n","type":"content","url":"/auxilliary-networks","position":1},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl2":"Pre-Auxilliary Networks"},"type":"lvl2","url":"/auxilliary-networks#pre-auxilliary-networks","position":2},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl2":"Pre-Auxilliary Networks"},"content":"\n\n","type":"content","url":"/auxilliary-networks#pre-auxilliary-networks","position":3},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl3":"Closed Condition: A Closer Look","lvl2":"Pre-Auxilliary Networks"},"type":"lvl3","url":"/auxilliary-networks#closed-condition-a-closer-look","position":4},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl3":"Closed Condition: A Closer Look","lvl2":"Pre-Auxilliary Networks"},"content":"Let’s first try fitting an Injective Network to a square using the PReLU function. Indeed as\n\n\ndiscussed before the PReLU activation does not guarantee non-self-intersection but\nwe use it to gain insights.\n\nfrom assets.networks import InjectiveNet\n\n# Generate target curve points\nnum_pts = 1000\nt = torch.linspace(0, 1, num_pts).reshape(-1, 1)\nX_t_square = square(num_pts)\n\nsquare_net = InjectiveNet(layer_count = 2, act_fn = nn.PReLU)\nautomate_training(\n    model = square_net, loss_fn = mse, X_train = t, Y_train = X_t_square,\n    learning_rate = 0.1, epochs = 1000, print_cost_every = 200\n)\n\nX_p_square = square_net(t)\nplot_curves(X_p_square, X_t_square)\n\nWe observe that the fit is really bad.\n\nThis is because the network first transforms the interval t\\in [0, 1] to a circle and then deeper\nlayers try to transform that to a square. This is not an easy task as the network has to transform\nthe circle arcs to the four straight edges.\n\nLet’s have a look at the first transformation as discussed in the section\n\n\nCondition 2 - Closed Curves:C(t) = \n\\begin{bmatrix}\ncos(2\\pi t)\\\\\nsin(2\\pi t)\n\\end{bmatrix}\n\nThe point [\\cos(2\\pi t), \\sin(2\\pi t)] is a point on the unit circle centered at the origin. What\nis happening here is that the open interval t \\in [0, 1] is transformed to a circle that makes\nsure that the first and the last point are the same and when fed into the network lead to the same\noutput point and hence create closed curves. This is explained visually in\n\n\nFigure 1.\n\n\n\nFigure 1:Transforming the line segment [0, 1] to the unit circle centered at the origin.\n\nWe could have also achieved the closed condition by transforming to a square instead of a circle.\nFor the above problem of fitting to a square this should help immensely as the Injective Network has\nto learn the identity mapping only! This is easy to do when using the PReLU activation.\n\nCode for transforming [0, 1] to unit square\n\ndef square_from_t(t):\n    num_points_per_edge = t.shape[0] // 4\n    # Generate points for each edge, including one corner point per edge\n    bottom = torch.stack([torch.linspace(-1, 1, num_points_per_edge + 1), -torch.ones(num_points_per_edge + 1)], dim=-1)[:-1]\n    right = torch.stack([torch.ones(num_points_per_edge + 1), torch.linspace(-1, 1, num_points_per_edge + 1)], dim=-1)[:-1]\n    top = torch.stack([torch.linspace(1, -1, num_points_per_edge + 1), torch.ones(num_points_per_edge + 1)], dim=-1)[:-1]\n    left = torch.stack([-torch.ones(num_points_per_edge + 1), torch.linspace(1, -1, num_points_per_edge + 1)], dim=-1)[:-1]\n\n    # Concatenate all edges in order\n    X = torch.cat([bottom, right, top, left], dim=0)\n    X = torch.vstack([X, X[0]])\n    X = torch.cat((X[:1], X[2:]), dim=0)\n    return X\n\nimport torch\nfrom torch import nn\n\nfrom assets.shapes import square_from_t\n\nclass InjectiveNet_SquareClosed(nn.Module):\n    def __init__(self, layer_count, act_fn):\n        super().__init__()\n\n        # Transform from t on the [0, 1] interval to unit square for closed shapes\n        self.closed_transform = square_from_t\n\n        layers = []\n        for i in range(layer_count):\n            layers.append(nn.Linear(2, 2))\n            layers.append(act_fn())\n        \n        self.linear_act_stack = nn.Sequential(*layers)\n    \n    def forward(self, t):\n        x = self.closed_transform(t)\n        x = self.linear_act_stack(x)\n        return x\n\n# Generate target curve points\nnum_pts = 1000\nt = torch.linspace(0, 1, num_pts).reshape(-1, 1)\nX_t_square = square(num_pts)\n\nsquare_net = InjectiveNet_SquareClosed(layer_count = 1, act_fn = nn.PReLU)\nautomate_training(\n    model = square_net, loss_fn = mse, X_train = t, Y_train = X_t_square,\n    learning_rate = 0.1, epochs = 1000, print_cost_every = 200\n)\n\nX_p_square = square_net(t)\nplot_curves(X_p_square, X_t_square)\n\n# Print model parameters after learning\nfor name, param in square_net.named_parameters():\n    print(f\"Layer: {name} | Values : {param[:2]} \\n\")\n\nObserve above that the network indeed learns the identity mapping as the linear transformation\nmatrix ends up as the identity matrix and the PReLU activation function ends up as x that is the\nidentity map with the slope as 1 and bias being 0.\n\n","type":"content","url":"/auxilliary-networks#closed-condition-a-closer-look","position":5},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl3":"Pre-Auxilliary Network","lvl2":"Pre-Auxilliary Networks"},"type":"lvl3","url":"/auxilliary-networks#pre-auxilliary-network","position":6},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl3":"Pre-Auxilliary Network","lvl2":"Pre-Auxilliary Networks"},"content":"Indeed we could have transformed to any simple closed curve first and then attached the neural\nnetwork layers after that. But since representing general simple closed curves is what we are trying\nto achieve, we can do a simpler thing. We transform the interval [0, 1] to a closed loop\nrepresented through polar coordinates in the form:\nr = f(\\theta), \\quad \\theta \\in [0, 2\\pi)\n\nwhere \\theta = 2\\pi t with t in [0, 1].\n\nWe can then use the vector [r \\cos(\\theta), r \\sin(\\theta)]^{T} as the first layer and attach\nthe usual Injective Network layers on top. This transformation from t to the vector\n[r \\cos(\\theta), r \\sin(\\theta)]^{T} is injective and satisfies the condition F(0) = F(1) and\nthus will create closed curves.\n\nThe function f can be represented using a full neural network with no additional constraints\nexcept that the output has to be positive. We call this network the Pre-Auxilliary Network.\n\nThe basic idea is that the Pre-Auxilliary Network will provide a favorable initial shape to the\nInjective Network and make its learning task simpler.\n\nclass PreAuxNet(nn.Module):\n    def __init__(self, layer_count, hidden_dim):\n        super().__init__()\n\n        # Pre-Auxilliary net needs closed transform to get same r at theta = 0, 2pi\n        self.closed_transform = lambda t: torch.hstack([\n            torch.cos(2 * torch.pi * t),\n            torch.sin(2 * torch.pi * t)\n        ])\n\n        layers = [nn.Linear(2, hidden_dim), nn.LazyBatchNorm1d(), nn.ReLU()]\n        for i in range(layer_count):\n            layers.append(nn.Linear(hidden_dim, hidden_dim))\n            layers.append(nn.LazyBatchNorm1d())\n            layers.append(nn.ReLU())\n        layers.append(nn.Linear(hidden_dim, 1))\n        layers.append(nn.ReLU())\n\n        self.forward_stack = nn.Sequential(*layers)\n    \n    def forward(self, t):\n        unit_circle = self.closed_transform(t) # Rows are cos(theta), sin(theta)\n        r = self.forward_stack(unit_circle)\n        x = r * unit_circle # Each row is now r*cos(theta), r*sin(theta)\n        return x\n\nclass PreAux_InjectiveNet(nn.Module):\n    def __init__(self, layer_count_inj, act_fn_inj, layer_count_preaux, hidden_dim_preaux):\n        super().__init__()\n\n        # Transform from t on the [0, 1] interval to unit square for closed shapes\n        self.closed_transform = PreAuxNet(layer_count_preaux, hidden_dim_preaux)\n\n        layers = []\n        for i in range(layer_count_inj):\n            layers.append(nn.Linear(2, 2))\n            layers.append(act_fn_inj())\n        \n        self.linear_act_stack = nn.Sequential(*layers)\n    \n    def forward(self, t):\n        x = self.closed_transform(t)\n        x = self.linear_act_stack(x)\n        return x\n\n# Generate target curve points\nnum_pts = 1000\nt = torch.linspace(0, 1, num_pts).reshape(-1, 1)\nX_t_square = square(num_pts)\n\nsquare_net = PreAux_InjectiveNet(\n    layer_count_inj = 1, act_fn_inj = nn.PReLU,\n    layer_count_preaux = 3, hidden_dim_preaux = 5\n)\nautomate_training(\n    model = square_net, loss_fn = mse, X_train = t, Y_train = X_t_square,\n    learning_rate = 0.01, epochs = 1000, print_cost_every = 200\n)\n\nX_p_square = square_net(t)\nplot_curves(X_p_square, X_t_square)\n\nGreat! Using a Pre-Auxilliary Network helps us build an Injective Network on top of a favorable\nclosed transform. This therefore helps us cover both the circle and the square fitting cases without\nspecifying a particular initial transform such as the circle or square. Pre-Auxilliary Networks are\ntherefore a way of adding to the capacity of Injective Networks using full scale neural networks.\n\n# Generate target curve points\nX_t_bunny = stanford_bunny(num_points = 1000)\nnum_pts = X_t_bunny.shape[0]\nt = torch.linspace(0, 1, num_pts).reshape(-1, 1)\n\nbunny_net = PreAux_InjectiveNet(\n    layer_count_inj = 1, act_fn_inj = nn.PReLU,\n    layer_count_preaux = 2, hidden_dim_preaux = 50\n)\nautomate_training(\n    model = bunny_net, loss_fn = mse, X_train = t, Y_train = X_t_bunny,\n    learning_rate = 0.1, epochs = 1000, print_cost_every = 200\n)\n\nX_p_bunny = bunny_net(t)\nplot_curves(X_p_bunny, X_t_bunny)\n\nWe now look at another technique that allows us to use full scale neural networks to augment the\npower of Injective Networks.\n\n","type":"content","url":"/auxilliary-networks#pre-auxilliary-network","position":7},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl2":"Post-Auxilliary Networks"},"type":"lvl2","url":"/auxilliary-networks#post-auxilliary-networks","position":8},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl2":"Post-Auxilliary Networks"},"content":"\n\n","type":"content","url":"/auxilliary-networks#post-auxilliary-networks","position":9},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl3":"More Representation Power: Augmenting in Polar Coordinates","lvl2":"Post-Auxilliary Networks"},"type":"lvl3","url":"/auxilliary-networks#more-representation-power-augmenting-in-polar-coordinates","position":10},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl3":"More Representation Power: Augmenting in Polar Coordinates","lvl2":"Post-Auxilliary Networks"},"content":"The Injective Network architecture can represent simple closed curves. But as we saw the requirement\nof network injectivity for imparting non-self-intersection to the parameterization restricted the\nhidden layer size to 2. Therefore the only thing we can control for increasing representation power\nis the network depth.\n\nThere is a way of working with polar coordinates where we can post-augment the parameterization with\na general neural network giving a boost to representation power. We discuss this technique now. But\nfirst we need to discuss the polar version of the Injective Network.\n\n","type":"content","url":"/auxilliary-networks#more-representation-power-augmenting-in-polar-coordinates","position":11},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl4":"Polar Neural Injective Curves","lvl3":"More Representation Power: Augmenting in Polar Coordinates","lvl2":"Post-Auxilliary Networks"},"type":"lvl4","url":"/auxilliary-networks#polar-neural-injective-curves","position":12},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl4":"Polar Neural Injective Curves","lvl3":"More Representation Power: Augmenting in Polar Coordinates","lvl2":"Post-Auxilliary Networks"},"content":"Consider first a polar setup of the parameterization similar to the cartesian case:\\begin{align*}\nr &= f(t)\\\\\n\\theta &= g(t), \\quad t \\in [0, 1)\n\\end{align*}\n\nAgain as before we look at a vector-valued equivalent:F: t \\rightarrow\n\\begin{bmatrix}\nr\\\\\n\\theta\n\\end{bmatrix}\n\nWe use the same network architecture as before with the only difference that now the output is\n[r, \\theta]^{T}:\n\n\n\nFigure 2:The network architecture for polar representation of simple closed curves looks deceptively similar\nto the one for cartesian coordinates with only the interpretation of the outputs being\n[r, \\theta]^{T} instead of [x, y]^{T}. But for the polar representation we need more constraints\nto hold to guarantee simple closed curves.\n\nThe network architecture for the polar representation looks deceptively similar to the one for\ncartesian coordinates with only the interpretation of the outputs being [r, \\theta]^{T} instead of\n[x, y]^{T}. But for the polar representation we need additional constraints to hold to guarantee\nsimple closed curves.\n\nPositive r\n\nThe polar representation requires that r is positive. The above architecture by default puts no\nrestriction on what values r can take. To generate only positive r values the activation\nfunction used at the last layer should be such that its outputs are always positive. Note that this\nactivation function should still be injective. Valid activation functions include sigmoid,\nsoftplus, modified tanh or ELU etc. An example modification of tanh would be to add 1 to it\nand use tanh(x) + 1 as the final activation function, this works as tanh is injective with range\n(-1, 1).\n\nRestricting \\theta \\in [0, 2 \\pi)\n\nOnce we make sure that we choose the right activation function that only outputs positive values\nwe need to make sure of the range of θ for a couple of reasons:\n\nThe first is related to self-intersection. The mapping from t \\to [r, \\theta] is injective.\nBut the curve it may trace out in the current form may self-intersect. Consider as an e.g. the\npoints [1, \\pi] and [1, 3\\pi]. These are different outputs and does not violate injectivity\nof the network but they correspond to the same point in the plane!\n\nThe second one is a representation problem. Let’s say we use the last layer activation\nfunction to be tanh + 1. This function has the range (0, 2) and therefore this will also be\nthe possible θ that we can generate. Thus, our curves will all be limited to\n\\theta \\in (0, 2). Of course, this is an artifact of using tanh + 1 and a different\nactivation function like the ELU + 1 would not have this issue, but it will suffer from the\nfirst issue above.\n\nTherefore the possible activation functions we can use at the last layer must be restricted to\nthose that can further be scaled to have a range [0, 2\\pi). One valid choice would be the\nfunction 2\\pi(tanh + 1).\n\nNote: There is then also the concern on the range of r\n\n","type":"content","url":"/auxilliary-networks#polar-neural-injective-curves","position":13},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl4":"Post-Auxilliary Networks","lvl3":"More Representation Power: Augmenting in Polar Coordinates","lvl2":"Post-Auxilliary Networks"},"type":"lvl4","url":"/auxilliary-networks#post-auxilliary-networks-1","position":14},{"hierarchy":{"lvl1":"Auxilliary Networks","lvl4":"Post-Auxilliary Networks","lvl3":"More Representation Power: Augmenting in Polar Coordinates","lvl2":"Post-Auxilliary Networks"},"content":"\n\nNow consider a polar representation of the form:\nr = h(\\theta), \\quad \\theta \\in [0, 2\\pi)\n\nAs we discussed before this is a guaranteed non-self-intersecting representation but suffers from\nthe general curve representation problem discussed in the section\n\n\nNaive Usage of Polar Coordinates: The Representation Power Problem.\n\nBut this can be a very powerful representation as we can use a full scale neural network to\nparameterize f(\\theta). We now see a way of combining the injective polar representation with this\nparameterization.\n\nNote: The final activation function of the neural network should be chosen such that the\noutputs are always positive to agree with r being positive.\n\nProbably the simplest thing to do would be to simply add the outputs of the two networks. One would\ndo this by adding the r value output from the auxilliary network to the different r values that\nare output from the injective network at any given θ. But performing this directly has a\nproblem. The auxilliary network is defined at all θ by definition but the injective network\nmay not output some values of θ and therefore we cannot add the two. This is shown in\n\n\nFigure 3.\n\n\n\nFigure 3:We cannot add the two networks at \\theta = \\pi since the injective network does not output all\npossible θ as t varies from [0, 1).\n\nBut there is a simple way of bypassing this problem. The basic idea is to use r values only at\nt values that lead to valid θ. The valid θ are generated as g(t). Therefore the\ncorresponding r = h(\\theta) = h(g(t)) are the r values generated at valid θ that occur as\nt is traversed from [0, 1).\n\nConsider a vector-valued mapping associated with the auxilliary network:A: t \\rightarrow\n\\begin{bmatrix}\nh(g(t))\\\\\n0\n\\end{bmatrix}\n\nNote this requires that we first feed in t into the injective network, get the corresponding\nθ and then feed that into the auxilliary network to generate the r for the A mapping.\n\nWe now have the following two vector-valued mappings:F: t \\rightarrow\n\\begin{bmatrix}\nf(t)\\\\\ng(t)\n\\end{bmatrix}\n\\quad\nA: t \\rightarrow\n\\begin{bmatrix}\nh(g(t))\\\\\n0\n\\end{bmatrix}\n\nConsider the addition of these:F + A: t \\rightarrow\n\\begin{bmatrix}\nf(t) + h(g(t))\\\\\ng(t)\n\\end{bmatrix}\n\nWe now need to prove that this mapping generates simple closed curves. This is quite intuitive and\nthe proof is as follows:\n\nClosed\n\nThe curves will be closed if (F + A)(0) = (F + A)(1). But we have\nF(0) = F(1) as F is the injective network. That is,\n\n  (f(0), g(0)) = (f(1), g(1))\n  \nWe have A(0) = (h(g(0)), 0) and A(1) = (h(g(1)), 0). Now since g(0) = g(1) we also have\nA(0) = A(1) and hence the curves will be closed.\n\nSimple\n\nThe curves will be simple if the mapping F + A is injective. Which is true if:\n\n  t_1 \\neq t_2 \\implies (F + A)(t_1) \\neq (F + A)(t_2)\n  \nWe start with the assumption that F + A is not injective. That is for some t_1 and t_2 the\noutputs are the same. Then we have:\n\n  g(t_1) &= g(t_2)\n  \nand\n\n  &f(t_1) + h(g(t_1)) = f(t_2) + h(g(t_2)) \\\\\n  \\implies &f(t_1) + h(g(t_1)) = f(t_2) + h(g(t_1)) \\\\\n  \\implies &f(t_1) = f(t_2)\n  \nBut this means that F(t_1) = F(t_2) which is a contradiction as F is the injective\nnetwork. Therefore our original assumption that F + A is not injective is false, and we\nconclude that the mapping F + A is indeed injective.\n\nFigure 4 shows the architecture for the augmented network\n\n\n\nFigure 4:First t is fed into the injective network to obtain its r_i and \\theta_i output. This\n\\theta_i is then fed into the auxilliary network to generate r_a which is then added to r_i to\ngenerate the final value.","type":"content","url":"/auxilliary-networks#post-auxilliary-networks-1","position":15},{"hierarchy":{"lvl1":"Neural Injective Geometry Showcase"},"type":"lvl1","url":"/curve-fit-showcase","position":0},{"hierarchy":{"lvl1":"Neural Injective Geometry Showcase"},"content":"# Basic imports\nimport torch\nfrom torch import nn\n\nfrom assets.shapes import square, circle, stanford_bunny\nfrom assets.loss_functions import mse\nfrom assets.utils import automate_training, plot_curves\n\n","type":"content","url":"/curve-fit-showcase","position":1},{"hierarchy":{"lvl1":"Neural Injective Geometry Showcase","lvl2":"Showcase"},"type":"lvl2","url":"/curve-fit-showcase#showcase","position":2},{"hierarchy":{"lvl1":"Neural Injective Geometry Showcase","lvl2":"Showcase"},"content":"\n\n","type":"content","url":"/curve-fit-showcase#showcase","position":3},{"hierarchy":{"lvl1":"Neural Injective Geometry Showcase","lvl3":"Heart","lvl2":"Showcase"},"type":"lvl3","url":"/curve-fit-showcase#heart","position":4},{"hierarchy":{"lvl1":"Neural Injective Geometry Showcase","lvl3":"Heart","lvl2":"Showcase"},"content":"\n\nfrom assets.shapes import heart\nfrom assets.networks import NIG_Net\n\n# Generate target curve points\nnum_pts = 1000\nt = torch.linspace(0, 1, num_pts).reshape(-1, 1)\nX_t = heart(num_pts)\n\nnig_net = NIG_Net(\n    layer_count_inj = 2, smm_num_grps = 6, smm_neurons_per_grp = 6\n)\nautomate_training(\n    model = nig_net, loss_fn = mse, X_train = t, Y_train = X_t,\n    learning_rate = 0.1, epochs = 1000, print_cost_every = 200\n)\n\nX_p = nig_net(t)\nplot_curves(X_p, X_t)\n\n","type":"content","url":"/curve-fit-showcase#heart","position":5},{"hierarchy":{"lvl1":"Neural Injective Geometry Showcase","lvl3":"Puzzle Piece","lvl2":"Showcase"},"type":"lvl3","url":"/curve-fit-showcase#puzzle-piece","position":6},{"hierarchy":{"lvl1":"Neural Injective Geometry Showcase","lvl3":"Puzzle Piece","lvl2":"Showcase"},"content":"\n\nfrom assets.shapes import puzzle_piece\nfrom assets.networks import NIG_Net\n\n# Generate target curve points\nnum_pts = 1000\nt = torch.linspace(0, 1, num_pts).reshape(-1, 1)\nX_t = puzzle_piece(num_pts)\n\nnig_net = NIG_Net(\n    layer_count_inj = 4, smm_num_grps = 10, smm_neurons_per_grp = 10\n)\nautomate_training(\n    model = nig_net, loss_fn = mse, X_train = t, Y_train = X_t,\n    learning_rate = 0.1, epochs = 1000, print_cost_every = 200\n)\n\nX_p = nig_net(t)\nplot_curves(X_p, X_t)\n\n","type":"content","url":"/curve-fit-showcase#puzzle-piece","position":7},{"hierarchy":{"lvl1":"Neural Injective Geometry Showcase","lvl3":"Hand","lvl2":"Showcase"},"type":"lvl3","url":"/curve-fit-showcase#hand","position":8},{"hierarchy":{"lvl1":"Neural Injective Geometry Showcase","lvl3":"Hand","lvl2":"Showcase"},"content":"\n\nfrom assets.shapes import hand\nfrom assets.networks import NIG_Net\n\n# Generate target curve points\nnum_pts = 1000\nt = torch.linspace(0, 1, num_pts).reshape(-1, 1)\nX_t = hand(num_pts)\n\nnig_net = NIG_Net(\n    layer_count_inj = 2, smm_num_grps = 6, smm_neurons_per_grp = 6\n)\nautomate_training(\n    model = nig_net, loss_fn = mse, X_train = t, Y_train = X_t,\n    learning_rate = 0.1, epochs = 1000, print_cost_every = 200\n)\n\nX_p = nig_net(t)\nplot_curves(X_p, X_t)\n\n","type":"content","url":"/curve-fit-showcase#hand","position":9},{"hierarchy":{"lvl1":"Neural Injective Geometry Showcase","lvl3":"Airplane","lvl2":"Showcase"},"type":"lvl3","url":"/curve-fit-showcase#airplane","position":10},{"hierarchy":{"lvl1":"Neural Injective Geometry Showcase","lvl3":"Airplane","lvl2":"Showcase"},"content":"\n\nfrom assets.shapes import airplane\nfrom assets.networks import NIG_Net\n\n# Generate target curve points\nnum_pts = 1000\nt = torch.linspace(0, 1, num_pts).reshape(-1, 1)\nX_t = airplane(num_pts)\n\nnig_net = NIG_Net(\n    layer_count_inj = 3, smm_num_grps = 6, smm_neurons_per_grp = 6\n)\nautomate_training(\n    model = nig_net, loss_fn = mse, X_train = t, Y_train = X_t,\n    learning_rate = 0.1, epochs = 1000, print_cost_every = 200\n)\n\nX_p = nig_net(t)\nplot_curves(X_p, X_t)","type":"content","url":"/curve-fit-showcase#airplane","position":11},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/extending-to-3d","position":0},{"hierarchy":{"lvl1":""},"content":"confusion hi confusion","type":"content","url":"/extending-to-3d","position":1},{"hierarchy":{"lvl1":"Motivation for Non-Self-Intersecting Geometry"},"type":"lvl1","url":"/introduction","position":0},{"hierarchy":{"lvl1":"Motivation for Non-Self-Intersecting Geometry"},"content":"","type":"content","url":"/introduction","position":1},{"hierarchy":{"lvl1":"Motivation for Non-Self-Intersecting Geometry","lvl2":"Introduction"},"type":"lvl2","url":"/introduction#introduction","position":2},{"hierarchy":{"lvl1":"Motivation for Non-Self-Intersecting Geometry","lvl2":"Introduction"},"content":"Shape optimization is the study of designing shapes that minimize or maximize some quantity of\ninterest. An example would be designing the wing of an airplane that maximizes the lift-to-drag\nratio. A typical shape optimization process loop consists of three steps:\n\n1. Shape Representation\n\nUsing a parameterization method to represent the shape. The parameters ϕ are the design\nvariables. E.g. splines with their control points as parameters.\n\n2. Shape Evaluation\n\nEvaluating some characteristic of the shape that is to be minimized or maximized. E.g. lift-to-drag\nratio of a wing.\n\n3. Shape Improvement\n\nChanging the parameters ϕ to design shapes that achieve better characteristics. E.g. gradient\ndescent to optimize ϕ.\n\nWe will focus on shape representation.\n\nIn a large class of shape optimization problems the shapes of interest are simple closed curves.\nSimple closed curves are curves that are closed, i.e. they form loops and are simple, i.e. they\ndo not self-intersect. Simple closed curves are also called Jordan curves.\n\nSimple closed curves have applications in a diverse set of fields: aerospace design, biomedical\nmodeling, computer graphics, scientific modelling and simulation, gaming, object recognition etc.\n\nTo understand why an optimization problem might be concerned only with simple closed curves consider\nthe following problem: Imagine that there is a flow going left \\to right around the body shown in\n\n\nFigure 1, and consider the lift-to-drag ratio of this shape. The\nflow only sees the boundary of this shape, the complicated spaghetti inside does not have any effect\nwhatsoever on the properties of the body. All the representation used to describe the inner curves\nis wasteful.\n\n\n\nFigure 1:An incoming flow would only interact with the boundary. The complicated internal mess is wasteful\nrepresentation.\n\nThis is especially crucial when doing optimization. Shape representation methods that can\npotentially represent self-intersecting shapes would cause the optimization algorithm to search a\nbigger design space than needed. Also, self-intersecting shapes might be physically undesirable or\nproblematic to deal with in downstream tasks such as shape evaluation. Thus, such a shape\nparameterization would require manual tuning during optimization. As an e.g. consider the\noptimization happening in \n\nFigure 2. We represent the shape\nusing 12 spline control points which are then fed into a neural network that predicts the shape’s\nlift-to-drag ratio. During training we used non-self-intersecting shapes as they are the ones of\ninterest, but during optimization, if after a gradient step the spline starts intersecting the\nnetwork struggles to predict its lift-to-drag ratio and steers the optimization in an even worse\ndirection. This is the classical distributional shift problem.\n\n\n\nFigure 2:Optimization starts from the dashed red initial airfoil shape which is iteratively modified. We see\nthat the optimization process suffers from distributional shift. Once a self-intersecting shape is\nreached it is iteratively made even worse.\n\nWhen using shape parameterization that can represent self-intersecting shapes the optimization\nalgorithm’s search has to be made limited, or someone has to sit and manually tune and check for\nself-intersection. Sometimes additional losses are added to the objective to prevent\nself-intersection. This is a hassle. In effect prevents an automated and aggressive shape space\nexploration. In an ideal setting, one would like to leave gradient descent running and go to sleep\nand wake up to find the optimal shape. Thus we need a shape parameterization that has the quality of\nnon-self-intersection in-built and for any set of parameters produces non-self-intersecting shapes\nonly.\n\n","type":"content","url":"/introduction#introduction","position":3},{"hierarchy":{"lvl1":"Motivation for Non-Self-Intersecting Geometry","lvl3":"Naive Usage of Polar Coordinates: The Representation Power Problem.","lvl2":"Introduction"},"type":"lvl3","url":"/introduction#naive-usage-of-polar-coordinates-the-representation-power-problem","position":4},{"hierarchy":{"lvl1":"Motivation for Non-Self-Intersecting Geometry","lvl3":"Naive Usage of Polar Coordinates: The Representation Power Problem.","lvl2":"Introduction"},"content":"One simple way of representing non-self-intersecting shapes would be through polar coordinates in\nthe form:\nr = f(\\theta), \\quad \\theta \\in [0, 2\\pi)\n\nwhere we would represent f using some parameterized form, e.g. neural networks.\n\nBut this representation cannot describe general simple closed curves. This is because r is\nparameterized as a function of θ and for a given θ we can only have one value of r.\nThis means we cannot represent shapes as shown in \n\nFigure 3.\n\n\n\nFigure 3:Since we have multiple r values at a single θ we cannot have a function that can represent\nthis.\n\nTherefore we would not be able to analyze a fish swimming in water as shown in \n\nFigure 4.\nOr if for whatever reason the optimal design was the \n\nStanford Bunny, we would not\nbe able to design for it (jokes apart we will use the Stanford Bunny as a target shape to test our\nshape representation techniques).\n\n\n\nFigure 4:A swimming fish with its body curved cannot be represented by a polar parameterization.\n\n\n\nFigure 5:The \n\nStanford Bunny. We will use it as\na target shape against which we test our parameterization techniques.\n\nWe now discuss a new method for shape parameterization that can describe general simple closed\ncurves. This method ensures that only non-self-intersecting curves are generated for any combination\nof the parameters. The central idea behind this geometry parameterization is the concept of\ninjective networks which we discuss next.","type":"content","url":"/introduction#naive-usage-of-polar-coordinates-the-representation-power-problem","position":5},{"hierarchy":{"lvl1":"Monotonic Networks"},"type":"lvl1","url":"/monotonic-networks","position":0},{"hierarchy":{"lvl1":"Monotonic Networks"},"content":"# Basic imports\nimport torch\nfrom torch import nn\n\nfrom assets.shapes import square, circle, stanford_bunny\nfrom assets.loss_functions import mse\nfrom assets.utils import automate_training, plot_curves\n\n","type":"content","url":"/monotonic-networks","position":1},{"hierarchy":{"lvl1":"Monotonic Networks","lvl2":"Creating Injective Functions with Parameters"},"type":"lvl2","url":"/monotonic-networks#creating-injective-functions-with-parameters","position":2},{"hierarchy":{"lvl1":"Monotonic Networks","lvl2":"Creating Injective Functions with Parameters"},"content":"\n\nWe observed earlier that with the \n\nPReLU activation Injective Networks\nperform much better. Therefore, the insight we gain that will help us power up Injective Networks is\n: add more parameters.\n\nOne way to add parameters to the Network is to use parameterized activation functions e.g.\n\\sigma(\\beta x) i.e. the sigmoid with a parameter β or try and create other activations.\nDuring the creation of these parameterized activations one needs to keep in mind that injectivity\nshould not be violated for any parameter value to ensure that the network always represents only\nsimple closed curves, which is our final goal.\n\nIt is quite difficult to combine activation functions together with appropriate parameters that add\nrepresentation power while maintaining injectivity. We will therefore look at a much more drastic\nand interesting approach.\n\nFirst, we need to understand how we could create a function that is injective. The first step is to\nrealize that:\n\nA continuous function is injective if and only if it is strictly monotonic.\n\nTherefore we can create injective functions by creating strictly monotonic functions.\n\nMath StackExchange,\n\n\nContinuous injective map is strictly monotonic\n\n","type":"content","url":"/monotonic-networks#creating-injective-functions-with-parameters","position":3},{"hierarchy":{"lvl1":"Monotonic Networks","lvl2":"Monotonic Networks: A Superpower"},"type":"lvl2","url":"/monotonic-networks#monotonic-networks-a-superpower","position":4},{"hierarchy":{"lvl1":"Monotonic Networks","lvl2":"Monotonic Networks: A Superpower"},"content":"\n\nWe will create injective activation functions using a drastic measure. Every activation function\nwill be an injective neural network!\n\nWe will create neural networks that take in a scalar input and output a scalar value and are\nstrictly monotonic in the input. Mathematically our activation function now is a neural network M\nM: \\mathbb{R} \\rightarrow \\mathbb{R}\n\nwhich satisfies one of:\nM(x_1) < M(x_2) \\quad \\forall x_1, x_2 \\in \\mathbb{R} \\quad s.t. \\; x_1 < x_2\\\\\nM(x_1) > M(x_2) \\quad \\forall x_1, x_2 \\in \\mathbb{R} \\quad s.t. \\; x_1 > x_2\n\nMonotonic Networks\n\nWe will choose neural networks that by design for any parameter value are always monotonic,\nthese are called Monotonic Networks.\n\nSill, 1997\n\nIgel, 2023\n\nWehenkel & Louppe, 2019\n\nKitouni et al., 2023\n\nRunje & Shankaranarayana, 2023\n\nEvery activation layer will be an independent Monotonic Network and all neurons will use the same\nneural network. This is shown in \n\nFigure 1.\n\n\n\nFigure 1:Injective Network with Monotonic Networks as activation functions. M_1 shown in the figure is a\nmonotonic network.\n\n","type":"content","url":"/monotonic-networks#monotonic-networks-a-superpower","position":5},{"hierarchy":{"lvl1":"Monotonic Networks","lvl3":"Building Monotonic Networks with Smooth Min-Max Networks","lvl2":"Monotonic Networks: A Superpower"},"type":"lvl3","url":"/monotonic-networks#building-monotonic-networks-with-smooth-min-max-networks","position":6},{"hierarchy":{"lvl1":"Monotonic Networks","lvl3":"Building Monotonic Networks with Smooth Min-Max Networks","lvl2":"Monotonic Networks: A Superpower"},"content":"Building Monotonic Networks is an active area of research and there are a few popular choices. The\nfirst Monotonic Network was designed by \n\nSill (1997) which used min and max operations to\nimpart monotonicity. But since the min-max operations are not differentiable they are hard\nto train and also suffer from dead neurons. Recently \n\nIgel (2023) proposed a\nvariation of the original Monotonic Networks proposed by \n\nSill (1997), which replaces the hard\nmin-max with their smooth variants. These are called Smooth Min-Max Monotonic Networks.\n\nWe will use Smooth Min-Max Monotonic Networks as activation functions to augment Injective Networks.\nWe use the code provided in Christian’s Repository\n\n\nhttps://​github​.com​/christian​-igel​/SMM.\n\nData Science StackExchange\n\n\nWhat is the “dying ReLU” problem in neural networks?\n\nimport numpy as np\nfrom assets.SmoothMonotonicNN import SmoothMonotonicNN\n\nclass NIG_Net(nn.Module):\n    def __init__(self, layer_count_inj, smm_num_grps, smm_neurons_per_grp):\n        super().__init__()\n\n        # Transformation from t on the [0, 1] interval to unit circle for closed shapes\n        self.closed_transform = lambda t: torch.hstack([\n            torch.cos(2 * torch.pi * t),\n            torch.sin(2 * torch.pi * t)\n        ])\n\n        self.layer_count_inj = layer_count_inj\n        self.linear_layers = nn.ModuleList()\n        self.monotonic_act = nn.ModuleList()\n\n        for i in range(layer_count_inj):\n            self.linear_layers.append(nn.Linear(2, 2))\n            self.monotonic_act.append(SmoothMonotonicNN(\n                n = 1, K = smm_num_grps, h_K = smm_neurons_per_grp, mask = np.array([1])\n            ))\n        \n    def forward(self, t):\n        x = self.closed_transform(t)\n\n        for linear_layer, monotonic_act in zip(self.linear_layers, self.monotonic_act):\n            # Apply linear transformation\n            x = linear_layer(x)\n            # Apply monotonic layers to each component of x separately\n            x1, x2 = x[:, 0:1], x[:, 1:2]  # Efficient slicing\n            x = torch.stack([monotonic_act(x1), monotonic_act(x2)], dim = -1)\n\n        return x\n\n# Generate target curve points\nX_t_bunny = stanford_bunny(num_points = 1000)\nnum_pts = X_t_bunny.shape[0]\nt = torch.linspace(0, 1, num_pts).reshape(-1, 1)\n\nbunny_net = NIG_Net(\n    layer_count_inj = 5, smm_num_grps = 6, smm_neurons_per_grp = 6\n)\nautomate_training(\n    model = bunny_net, loss_fn = mse, X_train = t, Y_train = X_t_bunny,\n    learning_rate = 0.1, epochs = 1000, print_cost_every = 200\n)\n\nX_p_bunny = bunny_net(t)\nplot_curves(X_p_bunny, X_t_bunny)\n\nWe observe an amazing improvement! Using Monotonic Networks as activation functions bolsters\nInjective Networks by introducing a lot of parameters which can be tuned to fit shapes with high\naccuracy.","type":"content","url":"/monotonic-networks#building-monotonic-networks-with-smooth-min-max-networks","position":7},{"hierarchy":{"lvl1":"Curve Similarity Measures"},"type":"lvl1","url":"/curve-similarity-measures","position":0},{"hierarchy":{"lvl1":"Curve Similarity Measures"},"content":"","type":"content","url":"/curve-similarity-measures","position":1},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl2":"Introduction"},"type":"lvl2","url":"/curve-similarity-measures#introduction","position":2},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl2":"Introduction"},"content":"Suppose we have a curve representation method, for example some parameterization scheme such as a\nspline. We also have a particular shape specified (say the \n\nStanford Bunny) and we\nwant our parameterization to represent that shape as closely as possible. To represent the given\nshape one way would be to tune the parameters of the representation scheme iteratively using a\ngradient descent optimization approach. This requires us to define an objective function that can\nthen be optimized over. An appropriate objective for this task would be some measure of\nsimilarity(or dissimilarity) between the target curve and the one traced out by our parameterization\n. The objective function can then be maximized(or minimized) to fit the parameters.\n\nThe target of this tutorial is to study curve similarity measures. We discuss different kinds of\nmeasures and also see their implementations.\n\n\n\nFigure 1:Parameterized shape, using a spline.\n\n\n\nFigure 5:Target shape, the \n\nStanford Bunny.\n\n","type":"content","url":"/curve-similarity-measures#introduction","position":3},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl2":"Concrete Problem Setup"},"type":"lvl2","url":"/curve-similarity-measures#concrete-problem-setup","position":4},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl2":"Concrete Problem Setup"},"content":"First we define the different objects that we deal with:\n\nShape Parameterization\n\nWe use some parameterization scheme e.g. splines to represent our shapes. We have a set of\nparameters ϕ that represent our shape. By changing ϕ we trace out different curves in the\nplane. We will think of ϕ as a column vector [\\phi_1, \\phi_2, \\ldots, \\phi_n]^{T}.\n\nParameterized Curve\n\nThis is the curve that is traced out by the parameterization scheme. We denote it by C_p and is\nobtained by sampling the scheme at different points along the actual curve. It is specified in the\nform of a N_p length sequence of (x, y) points. These points are ordered along the curve. We\nwill specify the points in a matrix in \\mathbb{R}^{N_p \\times 2} where each row corresponds to a\npoint (x_i, y_i). We denote the matrix as X_p.\n\nTarget Curve\n\nThis is the curve that we want our parameterization scheme to represent. We denote it by C_t and\nit is specified in the form of a N_t length sequence of (x, y) points. These points are ordered\nalong the curve. We will specify the points in a matrix in \\mathbb{R}^{N_t \\times 2} as we did for\nthe parameterized curve. We denote the matrix as X_t.\n\nLoss Function\n\nA function denoted as \\mathcal{L}(X_t, X_p) that measures the degree of dissimilarity between\nthe target curve and the parameterized curve. It should be differentiable to allow us to find\ngradients \\frac{d\\mathcal{L}}{d\\phi} that can then be used to run gradient descent.\n\nGoal: To tune ϕ such that our representation scheme traces out the target curve.\n\n","type":"content","url":"/curve-similarity-measures#concrete-problem-setup","position":5},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl2":"Similarity Measures"},"type":"lvl2","url":"/curve-similarity-measures#similarity-measures","position":6},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl2":"Similarity Measures"},"content":"We now discuss the different curve similarity measures. For each measure we describe the exact\nmathematical definition, practical considerations, modifications to make them differentiable and\nimplementations in \n\nPyTorch.\n\n","type":"content","url":"/curve-similarity-measures#similarity-measures","position":7},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl3":"Mean Squared Error (MSE)","lvl2":"Similarity Measures"},"type":"lvl3","url":"/curve-similarity-measures#mean-squared-error-mse","position":8},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl3":"Mean Squared Error (MSE)","lvl2":"Similarity Measures"},"content":"\n\n","type":"content","url":"/curve-similarity-measures#mean-squared-error-mse","position":9},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Description","lvl3":"Mean Squared Error (MSE)","lvl2":"Similarity Measures"},"type":"lvl4","url":"/curve-similarity-measures#description","position":10},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Description","lvl3":"Mean Squared Error (MSE)","lvl2":"Similarity Measures"},"content":"Assumption\n\nN_p = N_t = N. That is, we sample the parameterized curve at exactly N_t points.\n\nThe mean squared error loss function computes the average of the squared distance between the\ncorresponding points on the two curves. Mathematically,\n\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( d(X_{p}[i], X_{t}[i]) \\right)^2\n\nwhere, X[i] denotes ith row, i.e. (x_i, y_i) and d is a distance function.\n\nThough the measure is quite naive and not very robust, it is very simple and quick to implement and\nis also differentiable without any modifications.\n\n\n\nMean Squared Error (MSE) between two curves visualized.\n\n","type":"content","url":"/curve-similarity-measures#description","position":11},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Implementation","lvl3":"Mean Squared Error (MSE)","lvl2":"Similarity Measures"},"type":"lvl4","url":"/curve-similarity-measures#implementation","position":12},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Implementation","lvl3":"Mean Squared Error (MSE)","lvl2":"Similarity Measures"},"content":"import torch\n\ndef mse_loss(X_p, X_t):\n    # Calculate the squared Euclidean distances between corresponding rows\n    squared_distances = torch.sum((X_p - X_t) ** 2, dim = 1)\n\n    # Calculate mean of the squared distances to get the loss\n    loss = torch.mean(squared_distances)\n\n    return loss\n\n","type":"content","url":"/curve-similarity-measures#implementation","position":13},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl3":"Fourier Descriptor Matching","lvl2":"Similarity Measures"},"type":"lvl3","url":"/curve-similarity-measures#fourier-descriptor-matching","position":14},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl3":"Fourier Descriptor Matching","lvl2":"Similarity Measures"},"content":"\n\n","type":"content","url":"/curve-similarity-measures#fourier-descriptor-matching","position":15},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Description","lvl3":"Fourier Descriptor Matching","lvl2":"Similarity Measures"},"type":"lvl4","url":"/curve-similarity-measures#description-1","position":16},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Description","lvl3":"Fourier Descriptor Matching","lvl2":"Similarity Measures"},"content":"The idea behind Fourier descriptor matching is to compute the Fourier coefficients of both the\ntarget and the parameterized curve and then use the difference between them as the loss function.\n\nConcretely, given a curve we can approximate it using a complex Fourier series as follows :\nX(t) = \\sum_{n = -\\infty}^{\\infty} c_n e^{n 2 \\pi i t} \\quad t \\in [0, 1)\n\nIn Fourier descriptor matching we use the FFT algorithm to compute a finite number of coefficients\nc_n for each of the curves which have themselves been sampled(from X(t)) at a finite number of\npoints given by X_p and X_t. Let the coefficients be defined in vectors F_p and F_t. The\nloss is then computed as the mean squared error of the two coefficient vectors. If k is the total\nFourier coefficients computed in each FFT then the loss is given by:\n\\mathcal{L} = \\frac{1}{k} \\sum_{i=1}^{k} \\left( d(F_{p}[i], F_{t}[i]) \\right)^2\n\nNote\n\nFourier Descriptor Matching works only for closed curves. For open curves, it approximates with\na closed curve.\n\nNote\n\nFor the loss to be differentiable we require that the FFT be computed in a way that\nallows automatic differentiation to work.\n\n3Blue1Brown’s video on Fourier Series explains the formula really well.\n\n","type":"content","url":"/curve-similarity-measures#description-1","position":17},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Implementation","lvl3":"Fourier Descriptor Matching","lvl2":"Similarity Measures"},"type":"lvl4","url":"/curve-similarity-measures#implementation-1","position":18},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Implementation","lvl3":"Fourier Descriptor Matching","lvl2":"Similarity Measures"},"content":"import torch\nimport torch.fft\n\ndef fourier_descriptor_matching_loss(X_p, X_t, num_descriptors):\n    # Compute Fourier transforms (using FFT)\n    fft_p = torch.fft.fft(torch.complex(X_p[..., 0], X_p[..., 1]))\n    fft_t = torch.fft.fft(torch.complex(X_t[..., 0], X_t[..., 1]))\n\n    # Select relevant descriptors (low frequencies)\n    F_p = fft_p[:num_descriptors]\n    F_t = fft_t[:num_descriptors]\n\n    # Calculate MSE loss on magnitudes or complex values\n    loss = torch.mean(torch.abs(F_p - F_t)**2)\n    return loss\n\nThe implementation works because the FFT is differentiable in pytorch.\n\n","type":"content","url":"/curve-similarity-measures#implementation-1","position":19},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl3":"Hausdorff Distance","lvl2":"Similarity Measures"},"type":"lvl3","url":"/curve-similarity-measures#hausdorff-distance","position":20},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl3":"Hausdorff Distance","lvl2":"Similarity Measures"},"content":"\n\n","type":"content","url":"/curve-similarity-measures#hausdorff-distance","position":21},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Description","lvl3":"Hausdorff Distance","lvl2":"Similarity Measures"},"type":"lvl4","url":"/curve-similarity-measures#description-2","position":22},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Description","lvl3":"Hausdorff Distance","lvl2":"Similarity Measures"},"content":"Note: Most of the information is taken from the Wikipedia page\n\n\nHausdorff Distance.\n\nThe Hausdorff distance measures how far two subsets of a metric space are from each other.\nInformally, two sets are close in the Hausdorff distance if every point of either set is close to\nsome point of the other set. The Hausdorff distance is the longest distance someone can be forced to\ntravel by an adversary who chooses a point in one of the two sets, from where they then must travel\nto the other set. In other words, it is the greatest of all the distances from a point in one set to\nthe closest point in the other set.\n\nLet (M, d) be a metric space. For each pair of non-empty subsets X \\subset M and Y \\subset M,\nthe Hausdorff distance between X and Y is defined as\nd_{\\mathrm H}(X,Y) := \\max\\left\\{\\,\\sup_{x \\in X} d(x,Y),\\ \\sup_{y \\in Y} d(X,y) \\,\\right\\}\n\nwhere \\sup represents the supremum operator, \\inf the infimum operator, and where\nd(a, B) := \\inf_{b \\in B} d(a,b) quantifies the distance from a point a \\in X to the subset B\n\\subseteq X.\n\n\n\n","type":"content","url":"/curve-similarity-measures#description-2","position":23},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Differentiability","lvl3":"Hausdorff Distance","lvl2":"Similarity Measures"},"type":"lvl4","url":"/curve-similarity-measures#differentiability","position":24},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Differentiability","lvl3":"Hausdorff Distance","lvl2":"Similarity Measures"},"content":"The Hausdorff distance is by itself not differentiable if we implement using minimum and maximum\nfunctions. Therefore we need to work with approximations to it such as:\n\nSoft Hausdorff\n\nCompute the approximate Hausdorff distance by smoothing the minimum operation to ensure\ndifferentiability. In this approach we use a \n\nsmooth minimum function instead of\nthe minimum function directly.\n\nLet P_1 and P_2 be the sets of points on the two curves.\n\nUse a soft-minimum function to approximate the minimum distance between points, such as:\n\nd_{\\text{soft}}(p, P_2) = -\\tau \\log \\left( \\sum_{q \\in P_2} \\exp \\left( -\\frac{\\|p - q\\|_2^2}\n{\\tau} \\right) \\right)\n\nCompute the smoothed Hausdorff distance:\n\n\\text{Hausdorff}_{\\text{soft}}(P_1, P_2) = \\frac{1}{|P_1|} \\sum_{p \\in P_1}\nd_{\\text{soft}}(p, P_2) + \\frac{1}{|P_2|} \\sum_{q \\in P_2} d_{\\text{soft}}(q, P_1)\n\nLogSumExp - Smooth maximum\n\nThe LogSumExp (LSE) function is a smooth maximum – a smooth approximation to the maximum\nfunction. It is defined as the logarithm of the sum of the exponentials of the arguments:\n\\mathrm{LSE}(x_1, \\ldots, x_n) = \\log\\left( \\exp(x_1) + \\cdots + \\exp(x_n) \\right)\n\nWriting \\mathbf{x} = (x_1, \\ldots, x_n) the partial derivatives are:\n\\frac{\\partial}{\\partial x_i}{\\mathrm{LSE}(\\mathbf{x})} = \n\\frac{\\exp x_i}{\\sum_j \\exp {x_j}}\n\nwhich means the gradient of LogSumExp is the softmax function.\n\nSometimes, a parameter τ called the temperature parameter is used\n\\mathrm{LSE}(x_1, \\ldots, x_n) = \\tau \\log\\left( \\exp\\left(\\frac{x_1}{\\tau}\\right) + \\cdots +\n\\exp\\left(\\frac{x_n}{\\tau}\\right) \\right)\n\nτ controls the sharpness of the approximation. As τ approaches 0, the softmin\napproaches the true minimum.\n\nAlso, note that \\min \\left( x, y \\right) = -\\max \\left(-x, -y \\right). We can use this to get\nthe smooth minimum function using the LogSumExp.\n\nτ is called the temperature parameter because as \\tau \\to \\infty too\nmuch smoothing happens and all values tend to the same, i.e. at very high temperature behavior\nis random. If we cool down τ then the it tends to the true \\max function.\n\n","type":"content","url":"/curve-similarity-measures#differentiability","position":25},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Implementation","lvl3":"Hausdorff Distance","lvl2":"Similarity Measures"},"type":"lvl4","url":"/curve-similarity-measures#implementation-2","position":26},{"hierarchy":{"lvl1":"Curve Similarity Measures","lvl4":"Implementation","lvl3":"Hausdorff Distance","lvl2":"Similarity Measures"},"content":"import torch\n\ndef hausdorff_soft_loss(X_p, X_t, tau = 1.0):\n    # Compute pairwise squared distances\n    dist_matrix = torch.cdist(P1, P2) ** 2  # Shape: (N1, N2)\n    \n    # Compute soft-minimum distances\n    d_soft_p1_to_p2 = -tau * torch.logsumexp(-dist_matrix / tau, dim = 1)  # Shape: (N1,)\n    d_soft_p2_to_p1 = -tau * torch.logsumexp(-dist_matrix.T / tau, dim = 1)  # Shape: (N2,)\n\n    # Compute the soft Hausdorff loss\n    hausdorff_soft = d_soft_p1_to_p2.mean() + d_soft_p2_to_p1.mean()\n    \n    return hausdorff_soft.item()","type":"content","url":"/curve-similarity-measures#implementation-2","position":27},{"hierarchy":{"lvl1":"Injective Networks"},"type":"lvl1","url":"/the-idea-injective-net","position":0},{"hierarchy":{"lvl1":"Injective Networks"},"content":"%pip install numpy\n%pip install matplotlib\n%pip install torch\n\n","type":"content","url":"/the-idea-injective-net","position":1},{"hierarchy":{"lvl1":"Injective Networks","lvl2":"Neural Injective Curves"},"type":"lvl2","url":"/the-idea-injective-net#neural-injective-curves","position":2},{"hierarchy":{"lvl1":"Injective Networks","lvl2":"Neural Injective Curves"},"content":"We now discuss the abstract idea behind our shape parameterization to represent general simple\nclosed curves in the plane. As we shall see, this approach is conceptual and to actually create\ncurves a construction method is needed. Our method is based on neural networks with special\narchitectures that work as construction methods. We will see that the basic neural network\narchitecture is a very natural approach for constructing non-self-intersecting curves.\n\n","type":"content","url":"/the-idea-injective-net#neural-injective-curves","position":3},{"hierarchy":{"lvl1":"Injective Networks","lvl3":"Cartesian Neural Injective Curves","lvl2":"Neural Injective Curves"},"type":"lvl3","url":"/the-idea-injective-net#cartesian-neural-injective-curves","position":4},{"hierarchy":{"lvl1":"Injective Networks","lvl3":"Cartesian Neural Injective Curves","lvl2":"Neural Injective Curves"},"content":"\n\nConsider the following parameterization to represent curves:\\begin{align*}\nx &= f(t)\\\\\ny &= g(t), \\quad t \\in [0, 1]\n\\end{align*}\n\nThis representation can potentially represent any curve in the plane not necessarily simple and\nclosed.  Thus it is too broad, we want the functions f and g to be such that we only\nrepresent simple and closed curves.\n\n\n\nFigure 1:We will be concerned with simple closed curves, i.e. curves that loop back and do not\nself-intersect.\n\nSimple\n\nA curve is simple if it does not self-intersect. That is, two different t values should not map\nto the same point in the plane. Mathematically,\n\nt_1 \\neq t_2 \\implies (x(t_1), y(t_1)) \\neq (x(t_2), y(t_2))\n\nClosed\n\nA curve is closed if the starting and the ending points are the same. Mathematically,\n\n(x(0), y(0)) = (x(1), y(1))\n\nIf these additional constraints are satisfied by the functions f and g then the curves\ngenerated will be guaranteed to be simple closed curves. We now discuss a way of constructing\nfunctions f and g such that they always satisfy these constraints.\n\nTo create a construction mechanism we need to first look at the problem differently through a single\nvector-valued mapping instead of two scalar-valued functions f and g. Consider also, instead of\ntwo scalar values x and y, a single vector which has the two values stacked [x, y]^{T}.\nThis vector is parameterized as before through a parameter t. Concretely, consider a vector-valued\nmapping F that maps t to the vector of x and y.F: t \\rightarrow\n\\begin{bmatrix}\nx\\\\\ny\n\\end{bmatrix}\n\n","type":"content","url":"/the-idea-injective-net#cartesian-neural-injective-curves","position":5},{"hierarchy":{"lvl1":"Injective Networks","lvl4":"Condition 1 - Simple Curves","lvl3":"Cartesian Neural Injective Curves","lvl2":"Neural Injective Curves"},"type":"lvl4","url":"/the-idea-injective-net#condition-1-simple-curves","position":6},{"hierarchy":{"lvl1":"Injective Networks","lvl4":"Condition 1 - Simple Curves","lvl3":"Cartesian Neural Injective Curves","lvl2":"Neural Injective Curves"},"content":"For the curves represented through the mapping F to be simple, we need F to be an\n\n\ninjective map as then every t value will map\nto a different point in the plane and does not self-intersect. We will now see how neural networks\nare quite naturally suited for this.\n\nConsider the specific neural network architecture shown in\n\n\nFigure 2:\n\n\n\nFigure 2:If each layer is injective then the composition of these i.e. the entire network will also be\ninjective.\n\nWe now see each transformation of the network one by one. At the first layer the mapping from t to\na vector At of \\dim 2 is performed by a column vector A and is one to one. The next mapping\nby a non-linear activation function \\sigma(z) is an element-wise mapping. If we choose our\nactivation function to be an injective function, such as the sigmoid, PReLU, ELU etc. then the\nmapping it produces from one vector to the next will also be injective. Note here that we cannot use\nthe ReLU as it is not an injective function. Now consider the mapping from a \\dim 2 vector to\nanother produced by some 2 \\times 2 matrix B. This mapping is injective with probability 1!\n. That is, if we generate a random 2 \\times 2 matrix B then it is\ninjective with probability 1. The neural network is composed of these linear and non-linear mappings\nin a sequence. Since the composition of two injective mappings is injective,\nthe entire network is injective as a whole. Thus the mapping F from t to [x, y]^{T} is an\ninjective map. Hence, the curves produced by it will be simple curves.\n\nCrucial: Hidden layer size 2\n\nA neural network constructed as above is injective with probability 1. The critical information\nused in the construction of the network was that we maintain a hidden layer size of 2 which leads to\nthe matrices B being injective. If we had produced larger hidden layers, say of size m, then we\nwould eventually have to reduced the size back to 2 because our output is a \\dim 2 vector\nrepresenting x and y and a m \\times 2 matrix that would produce this would not be injective in\nmost cases.\n\nA square n \\times n matrix is not injective if and only if it is\nnot invertible.\n\nSingular matrices are rare in the sense that if a square matrix’s entries are randomly\nselected from any bounded region on the number line or complex plane, the probability that the\nmatrix is singular is 0, that is, it will “almost never” be singular.\n\nWikipedia, Invertible Matrix\n\nComposition of two injective functions f and g, f \\circ g is also\ninjective.\n\n\nWikipedia, Injective function\n\n","type":"content","url":"/the-idea-injective-net#condition-1-simple-curves","position":7},{"hierarchy":{"lvl1":"Injective Networks","lvl4":"Condition 2 - Closed Curves","lvl3":"Cartesian Neural Injective Curves","lvl2":"Neural Injective Curves"},"type":"lvl4","url":"/the-idea-injective-net#closed-transformation","position":8},{"hierarchy":{"lvl1":"Injective Networks","lvl4":"Condition 2 - Closed Curves","lvl3":"Cartesian Neural Injective Curves","lvl2":"Neural Injective Curves"},"content":"For the curves represented through F to be closed, we need the x and y values to be the same\nat t = 0, 1. Note, that this has to be in-built into the mapping without affecting the injectivity\nproperty that we achieved before. One way is to first produce the transformation:C(t) = \n\\begin{bmatrix}\ncos(2\\pi t)\\\\\nsin(2\\pi t)\n\\end{bmatrix}\n\nThis mapping is injective for t \\in [0, 1) and also satisfies C(0) = C(1). Therefore, we use\nthis as the first transformation before applying the linear and non-linear activation layers. The\nmodified neural network architecture is shown in\n\n\nFigure 3:\n\n\n\nFigure 3:This modified neural network architecture produces closed curves. Also, since the entire network is\ninjective the curves produced will also be non-self-intersecting.\n\nWe now have F(0) = F(1) and the mapping F is also injective with probability 1!\n\nWe call this architecture the Injective Network.\n\n","type":"content","url":"/the-idea-injective-net#closed-transformation","position":9},{"hierarchy":{"lvl1":"Injective Networks","lvl4":"Implementation","lvl3":"Cartesian Neural Injective Curves","lvl2":"Neural Injective Curves"},"type":"lvl4","url":"/the-idea-injective-net#implementation","position":10},{"hierarchy":{"lvl1":"Injective Networks","lvl4":"Implementation","lvl3":"Cartesian Neural Injective Curves","lvl2":"Neural Injective Curves"},"content":"We first do a basic implementation of an Injective Network.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\ndef injective_net(t):\n    # Perform the transformation for closed curves\n    z = np.hstack([np.cos(2 * np.pi * t), np.sin(2 * np.pi * t)])\n    a = np.tanh(z)\n\n    # Perform the linear and non-linear transformations\n    layer_count = 2\n    for i in range(layer_count):\n        # We subtract 0.5, without it the shapes will be extremely thin like lines\n        A = np.random.random((2, 2)) - 0.5\n        z = a @ A\n        a = np.tanh(z)\n    \n    return a\n\nnum_pts = 1000\nt = np.linspace(0, 1, num_pts).reshape(-1, 1)\n\nX = injective_net(t)\n\nplt.plot(X[:, 0], X[:, 1])\nplt.show()\n\nNow we generate random shapes to get a flavor of the variety of shapes that the parameterization can\nrepresent. Click the Generate Curve button below to initialize random A matrices and\nvisualize the resulting curves. The activation function used is LeakyReLU and it’s a 2 layer deep\nInjective Network.\n\nAlso, notice that the generated curve never self intersects.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import Button, VBox, Output, Layout\nfrom IPython.display import display\n\ndef LeakyReLU(x, alpha = 0.1):\n    return np.maximum(alpha * x, x)\n\ndef injective_net(t):\n    # Perform the transformation for closed curves\n    z = np.hstack([np.cos(2 * np.pi * t), np.sin(2 * np.pi * t)])\n    a = LeakyReLU(z)\n\n    # Perform the linear and non-linear transformations\n    layer_count = 2\n    for i in range(layer_count):\n        A = np.random.random((2, 2)) - 0.5\n        z = a @ A\n        a = LeakyReLU(z)\n    \n    return a\n\n# Create an output widget to display the plot\noutput = Output()\n\n# Function to execute when the button is clicked\ndef generate_plot(_):\n    with output:\n        output.clear_output(wait = True)  # Clear previous output for fresh plot\n        num_pts = 1000\n        t = np.linspace(0, 1, num_pts).reshape(-1, 1)\n\n        X = injective_net(t)\n\n        # Plot the result\n        plt.plot(X[:, 0], X[:, 1])\n        plt.show()\n\n# Create a button widget\nbutton = Button(\n    description = \"Generate Curve\",\n)\nbutton.on_click(generate_plot)\n\n# Display the button and output together\ndisplay(VBox([button, output]))\n\n","type":"content","url":"/the-idea-injective-net#implementation","position":11},{"hierarchy":{"lvl1":"Injective Networks","lvl3":"Fitting Target Curves: Curve Similarity Metrics","lvl2":"Neural Injective Curves"},"type":"lvl3","url":"/the-idea-injective-net#fitting-target-curves-curve-similarity-metrics","position":12},{"hierarchy":{"lvl1":"Injective Networks","lvl3":"Fitting Target Curves: Curve Similarity Metrics","lvl2":"Neural Injective Curves"},"content":"Now that we have a shape parameterization method that represents only simple closed curves, we would\nlike to fit it to target shapes. This is useful in various scenarios: starting an optimization from\na particular initial shape, exploring the representative power of a given depth network, the effect\nof activation functions on the types of shapes that we can represent etc.\n\nLet’s say we have a target shape specified (say the Stanford Bunny) and we want our\nparameterization to represent that shape as closely as possible. To represent the given shape one\nway would be to tune the parameters of the representation scheme iteratively using a gradient descent\noptimization approach. This requires us to define an objective function that can then be optimized\nover. An appropriate objective for this task would be some measure of similarity(or dissimilarity)\nbetween the target curve and the one traced out by our parameterization. The objective function can\nthen be maximized(or minimized) to fit the parameters.\n\nFor details on similarity metrics refer to the tutorial on \n\nCurve Similarity Measures.\n\nHere we will assume that we have some curve similarity measures available to us defined as loss\nfunctions in PyTorch. We will use these directly to fit shapes. In particular we have the following\nsetup:\n\nTable 1:The optimization problem setup\n\nObject\n\nDetails\n\nShape Parameterization\n\nInjective Network with parameters ϕ\n\nParametrized Curve\n\nSampled at discrete t \\in [0, 1) from Injective Network, stored in N_p\\times 2 matrix X_p\n\nTarget Curve\n\nSpecified curve to fit, stored in N_t \\times 2 matrix X_t\n\nLoss Function\n\n\\mathcal{L}(X_t, X_p)\n\nNote that since X_p is sampled from the network with parameters ϕ it is also a function of\nϕ, X_p = X_p(\\phi).\n\nGoal: Use automatic differentiation to get gradients of \\mathcal{L}(X_t, X_p) w.r.t.\nϕ and then run gradient descent and tune the parameters.\n\nThe Stanford Bunny\n\nThe \n\nStanford Bunny. We will use it as\na target shape against which we test our parameterization techniques.\n\n","type":"content","url":"/the-idea-injective-net#fitting-target-curves-curve-similarity-metrics","position":13},{"hierarchy":{"lvl1":"Injective Networks","lvl4":"Implementation","lvl3":"Fitting Target Curves: Curve Similarity Metrics","lvl2":"Neural Injective Curves"},"type":"lvl4","url":"/the-idea-injective-net#implementation-1","position":14},{"hierarchy":{"lvl1":"Injective Networks","lvl4":"Implementation","lvl3":"Fitting Target Curves: Curve Similarity Metrics","lvl2":"Neural Injective Curves"},"content":"\n\nimport torch\nfrom torch import nn\n\nclass InjectiveNet(nn.Module):\n    def __init__(self, layer_count, act_fn):\n        super().__init__()\n\n        # Define the transformation from t on the [0, 1] interval to unit circle for closed shapes\n        self.closed_transform = lambda t: torch.hstack([\n            torch.cos(2 * torch.pi * t),\n            torch.sin(2 * torch.pi * t)\n        ])\n\n        layers = []\n        for i in range(layer_count):\n            layers.append(nn.Linear(2, 2))\n            layers.append(act_fn())\n        \n        self.linear_act_stack = nn.Sequential(*layers)\n    \n    def forward(self, t):\n        x = self.closed_transform(t)\n        x = self.linear_act_stack(x)\n        return x\n\nNow we train Injective Networks to fit target shapes. In particular we use shapes from the\n\n\nshapes python module and loss functions from the\n\n\nloss_functions module. We will use a square\nand circle as target shapes and the Mean Squared Error (MSE) loss function to quantify the\ndifference between the parameterized and the target curve.\n\nWe also use the \n\nautomate_training function to\nhide the training details and focus on the parameterization. The \n\nplot_curve\nfunction plots the target and the parameterized curves and abstracts away the plotting details.\n\nThe code used for automating the training of Networks is:\n\nimport torch\nfrom torch import nn\n\n\ndef automate_training(\n        model,\n        loss_fn,\n        X_train,\n        Y_train,\n        epochs = 1000,\n        print_cost_every = 200,\n        learning_rate = 0.001,\n):\n    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.99)\n\n    num_digits = len(str(epochs))\n\n    for epoch in range(epochs):\n        Y_model = model(X_train)\n        loss = loss_fn(Y_model, Y_train)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        scheduler.step(loss.item())\n\n        if epoch == 0 or (epoch + 1) % print_cost_every == 0:\n            print(f'Epoch: [{epoch + 1:{num_digits}}/{epochs}]. Loss: {loss.item():11.6f}')\n\nfrom assets.shapes import circle, square\nfrom assets.loss_functions import mse\nfrom assets.utils import automate_training, plot_curves\n\n# Generate target curve points\nnum_pts = 1000\nt = torch.linspace(0, 1, num_pts).reshape(-1, 1)\nX_t_circle = circle(num_pts)\nX_t_square = square(num_pts)\n\n# Initialize networks to learn the target shapes and train\ncircle_net = InjectiveNet(layer_count = 3, act_fn = nn.Tanh)\nsquare_net = InjectiveNet(layer_count = 3, act_fn = nn.Tanh)\n\nprint('Training Circle Net:')\nautomate_training(\n    model = circle_net, loss_fn = mse, X_train = t, Y_train = X_t_circle,\n    learning_rate = 0.1, epochs = 1000, print_cost_every = 200\n)\nprint('Training Square Net:')\nautomate_training(\n    model = square_net, loss_fn = mse, X_train = t, Y_train = X_t_square,\n    learning_rate = 0.1, epochs = 1000, print_cost_every = 200\n)\n\n# Get final curve represented by the networks\nX_p_circle = circle_net(t)\nX_p_square = square_net(t)\n\n# Plot the curves\nplot_curves(X_p_circle, X_t_circle)\nplot_curves(X_p_square, X_t_square)\n\nAwesome! We see that we are doing good on simple shapes. Now we move onto something challenging.\n\nLet’s test the method on the Stanford Bunny. We will treat the Stanford Bunny as our recurring\nevaluation target for different architectures. Therefore we will use a fixed number of points on the\ntarget bunny curve to maintain consistency across curve fitting results.\n\nWe now try different layer_count and act_fn to try and fit the bunny. We recommend you to play\naround with the code below to get a good feel for curve fitting capacity of Injective Networks.\n\nfrom assets.shapes import stanford_bunny\n\n# Generate target curve points\nX_t_bunny = stanford_bunny(num_points = 1000)\nnum_pts = X_t_bunny.shape[0]\nt = torch.linspace(0, 1, num_pts).reshape(-1, 1)\n\nbunny_net = InjectiveNet(layer_count = 5, act_fn = nn.LeakyReLU)\n\nautomate_training(bunny_net, mse, t, X_t_bunny, learning_rate = 0.1, epochs = 1000, print_cost_every = 200)\n\nX_p_bunny = bunny_net(t)\nplot_curves(X_p_bunny, X_t_bunny)\n\nWe observe very poor performance❗⚠️\n\nIf you played around with the code above you would have observed that Injective Networks on their\nown:\n\ncan get stuck in local optima.\n\ncan fail to be expressive enough.\n\nTo get ideas on improving the parameterization we first “cheat” a little and use the\nPReLU activation function. PReLU has a learnable slope parameter α which can\nduring training become negative and therefore violate the injectivity property required from the\nactivation function for non-self intersection. Therefore, in general PReLU cannot be used in\nInjective Networks but we use it here to gain insights into improving the method further.\n\nPyTorch PReLU\n\nApplies the element-wise PReLU function.\\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)\n\nor\\text{PReLU}(x) =\n\\begin{cases}\nx, & \\text{ if } x \\ge 0 \\\\\nax, & \\text{ otherwise }\n\\end{cases}\n\nHere a is a learnable parameter. When called without arguments, nn.PReLU() uses a single\nparameter a across all input channels.\n\nfrom assets.shapes import stanford_bunny\n\n# Generate target curve points\nX_t_bunny = stanford_bunny(num_points = 100)\nnum_pts = X_t_bunny.shape[0]\nt = torch.linspace(0, 1, num_pts).reshape(-1, 1)\n\nbunny_net = InjectiveNet(layer_count = 5, act_fn = nn.PReLU)\n\nautomate_training(bunny_net, mse, t, X_t_bunny, learning_rate = 0.1, epochs = 10000, print_cost_every = 2000)\n\nX_p_bunny = bunny_net(t)\nplot_curves(X_p_bunny, X_t_bunny)\n\nWe see an exciting improvement! Injective Networks with PReLU do much better. This gives us the\nfollowing insight:\n\nInjective Networks are parameter deficient.\n\nAdding parameters gives them much more representation power.\n\nWe will come back to address this in.ajlsdjfklasdkf\nBut for now we look at auxilliary networks.","type":"content","url":"/the-idea-injective-net#implementation-1","position":15}]}